{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "pyspark-study",
      "private_outputs": true,
      "provenance": [],
      "authorship_tag": "ABX9TyM3I2z/W7sz5yWmr8rnZYJa",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DuplamenteH/spark-study/blob/main/pyspark_study.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NioqJuE8zf9m"
      },
      "source": [
        "instalação do pyspark no colab\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E84-KusQ-isn"
      },
      "source": [
        "para instalar o pyspark primeiro teremos que instalar  -> java , spark\n",
        "configurar java_home, e spark_home, instalar o findspark"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tfhYta8e_8d-"
      },
      "source": [
        "instalação de configuração das variaveis de ambiente"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JofZ5SXlzI3G"
      },
      "source": [
        "\n",
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "!wget -q https://archive.apache.org/dist/spark/spark-3.0.1/spark-3.0.1-bin-hadoop2.7.tgz\n",
        "!tar xf spark-3.0.1-bin-hadoop2.7.tgz\n",
        "!pip install -q findspark"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d47dMR9MA6wW"
      },
      "source": [
        "import findspark"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iPKmBu4GABch"
      },
      "source": [
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.0.1-bin-hadoop2.7\"\n",
        " \n",
        "# tornar o pyspark \"importável\"\n",
        "#import findspark\n",
        "findspark.init('spark-3.0.1-bin-hadoop2.7')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CwjKVJY3CHMc"
      },
      "source": [
        "Depois de importado e configurado vamos dar inicio ao spark"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6ieIx_WaCWNp"
      },
      "source": [
        "importaremos sparkSession para inicio a nossa sessão"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mi0Kg6D2Aydv"
      },
      "source": [
        "from pyspark.sql import SparkSession"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EE3ACaSQCVI-"
      },
      "source": [
        "spark = SparkSession.builder.appName(\"Data_Wrangling\").getOrCreate()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ccV9tHHRGPLz"
      },
      "source": [
        "!wget -q https://raw.githubusercontent.com/Apress/applied-data-science-using-pyspark/main/Ch02/Chapter2_Data/movie_data_part1.csv"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_4eCKz4iEMzD"
      },
      "source": [
        "dados = 'movie_data_part1.csv'\n",
        "file_type = \"csv\"\n",
        "infer_schema = \"False\"\n",
        "first_row_is_header = \"True\"\n",
        "delimiter  = \"|\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mrfY9gUKGwV8"
      },
      "source": [
        "primeira forma de carregar os dados"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "94DBUnc3ErC8"
      },
      "source": [
        "df = spark.read.format(file_type)\\\n",
        ".option(\"inferSchema\",infer_schema)\\\n",
        ".option(\"header\",first_row_is_header)\\\n",
        ".option(\"sep\",delimiter)\\\n",
        ".load(dados)\n",
        "\n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kj7AfpyQG0dA"
      },
      "source": [
        "Lendo os dados de uma tabela hive\n",
        "\n",
        "\n",
        "df2 = spark.sql(\"select * from database.table_name\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_wiKBH_WHN9z"
      },
      "source": [
        "printamos os schema, assim podemos ver as colunas, se existem valores faltantes, e o tipo de cada coluna"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "32WyRIu0HA13"
      },
      "source": [
        "df.printSchema()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kV9U2xCfHhDg"
      },
      "source": [
        "com a função df.dtypes, retornamos uma lista de tuplas com o nome da coluna e o tipo da coluna apenas"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FFgJ3mbaHKnS"
      },
      "source": [
        "df.dtypes"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3fR1v7ULHsHA"
      },
      "source": [
        "agr vamos saber o tamanho desse dataset, quantas linha tem o mesmo"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V_MyUosfHcs0"
      },
      "source": [
        "df.count()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OUxjXVBaH0Kp"
      },
      "source": [
        "uma maneira mai bonita seria assim:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hapVd0RXHzCx"
      },
      "source": [
        "print(\"O total dos nossos registros é de  : {}\".format(df.count()))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EzBHDFycIi3Q"
      },
      "source": [
        "Coletando subcolunas para ter uma visão geral dos dados"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nmJeA20XIH2r"
      },
      "source": [
        "select_columns = ['id', 'budget', 'popularity', 'release_date',\n",
        "                  'revenue', 'title']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "08WeikyaJBel"
      },
      "source": [
        "df =df.select(*select_columns)\n",
        "df.show(30,False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FudZKVAjMD3u"
      },
      "source": [
        "Valores faltantes do nosso dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JH5zXjbtMHN-"
      },
      "source": [
        "algumas maneiras de calcular os valores faltantes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QpLu-9WSJDYx"
      },
      "source": [
        "from pyspark.sql.functions import *"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "taeu3JefMTUa"
      },
      "source": [
        "df.filter((df['popularity']=='')|df['popularity'].isNull()|isnan(df['popularity'])).count()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JqfNAotfMtcG"
      },
      "source": [
        "na operação usando o condicional OR -> |,\n",
        "\n",
        "na primeira condição achamos as strings nulas presentes em popularity comparando com strings vazias, na segunda condição chamamos a função .isNull() para a colunar popularity que retorna true quando alguma linha popularity é verdadeira e false se o contrario acontecer, na terceira condição usando a função isnan()-> not a number."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0SIHSsLTNpRV"
      },
      "source": [
        "A segunda forma de achar a quantidade de valores faltantes é da seguinte forma."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yk1SiMgzMqBq"
      },
      "source": [
        "df.select([count(when((col(c)=='')| col(c).isNull()|isnan(c),c)).\n",
        "           alias(c) for c in df.columns]).show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gOEQ46I6ObsF"
      },
      "source": [
        "#Pagina 64 // One-Way-Frequencies"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vqCcoXyjOG9T"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}